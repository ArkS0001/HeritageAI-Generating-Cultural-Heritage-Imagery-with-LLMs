{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8/LTVS+3AIfct5RhLVnjB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArkS0001/HeritageAI-Generating-Cultural-Heritage-Imagery-with-LLMs/blob/main/PlanB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz6-gwbZcXxd",
        "outputId": "b47ed7ee-b5cd-4fba-a6f5-3e2fca349809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python numpy torch torchvision matplotlib gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vrindaprabhu/deepfillv2_colab.git\n",
        "!gdown \"https://drive.google.com/u/0/uc?id=1uMghKl883-9hDLhSiI8lRbHCzCmmRwV-&export=download\"\n",
        "!mv /content/deepfillv2_WGAN_G_epoch40_batchsize4.pth deepfillv2_colab/model/deepfillv2_WGAN.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt5offKbeRpa",
        "outputId": "f7edf2bb-5161-4590-dd16-c4a424f2b48b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deepfillv2_colab'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 99 (delta 2), reused 1 (delta 1), pack-reused 96\u001b[K\n",
            "Receiving objects: 100% (99/99), 571.56 KiB | 510.00 KiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1uMghKl883-9hDLhSiI8lRbHCzCmmRwV-&export=download\n",
            "To: /content/deepfillv2_WGAN_G_epoch40_batchsize4.pth\n",
            "100% 64.8M/64.8M [00:00<00:00, 74.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import gdown\n",
        "\n",
        "# Define the DeepFill v2 generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.middle = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=4, dilation=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=8, dilation=8),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=16, dilation=16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.middle(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert to RGB\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # Resize to 256x256\n",
        "    image = cv2.resize(image, (256, 256))\n",
        "    # Create a mask (this is a simple example, a real mask should come from the user)\n",
        "    mask = np.zeros_like(image)\n",
        "    mask[100:150, 100:150, :] = 1  # Simulate some missing area\n",
        "    # Normalize and convert to tensors\n",
        "    image = transforms.ToTensor()(image)\n",
        "    mask = transforms.ToTensor()(mask)\n",
        "    # Concatenate image and mask\n",
        "    image_with_mask = torch.cat((image, mask), dim=0)\n",
        "    return image, mask, image_with_mask.unsqueeze(0)\n",
        "\n",
        "# Function to generate an image using the generator\n",
        "def generate_image(generator, image_with_mask):\n",
        "    with torch.no_grad():\n",
        "        generated_image = generator(image_with_mask).cpu()\n",
        "    return generated_image\n",
        "\n",
        "# Main function\n",
        "def main(image_path, model_path):\n",
        "    # Preprocess the image\n",
        "    original_image, mask, image_with_mask = preprocess_image(image_path)\n",
        "\n",
        "    # Initialize and load the pre-trained generator\n",
        "    generator = Generator()\n",
        "    generator.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    generator.eval()\n",
        "\n",
        "    # Generate the reconstructed image\n",
        "    reconstructed_image = generate_image(generator, image_with_mask)\n",
        "\n",
        "    # Save and display the generated image\n",
        "    save_image(reconstructed_image, 'reconstructed_image.png')\n",
        "\n",
        "    # Display the original, mask, and reconstructed images\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title('Original Image')\n",
        "    plt.imshow(np.transpose(original_image.numpy(), (1, 2, 0)))\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title('Mask')\n",
        "    plt.imshow(np.transpose(mask.numpy(), (1, 2, 0)))\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title('Reconstructed Image')\n",
        "    plt.imshow(np.transpose(reconstructed_image[0].numpy(), (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Set paths to the image and model\n",
        "image_path = '/content/testPLAN-B.jpg'\n",
        "model_path = '/content/deepfillv2_colab/model/deepfillv2_WGAN.pth'\n",
        "\n",
        "# Download the pre-trained model if not already downloaded\n",
        "# gdown.download('https://drive.google.com/uc?id=1qdWbW0_0XBIkq2PZBdlrwbpa7GnGZBF7', model_path, quiet=False)\n",
        "\n",
        "# Run the main function\n",
        "main(image_path, model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "QqHCF6jPdDec",
        "outputId": "03622a95-d0ae-43b4-9f9d-a36e3e86d51b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.2.weight\", \"encoder.2.bias\", \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.6.weight\", \"encoder.6.bias\", \"encoder.8.weight\", \"encoder.8.bias\", \"middle.0.weight\", \"middle.0.bias\", \"middle.2.weight\", \"middle.2.bias\", \"middle.4.weight\", \"middle.4.bias\", \"middle.6.weight\", \"middle.6.bias\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\", \"decoder.4.weight\", \"decoder.4.bias\", \"decoder.6.weight\", \"decoder.6.bias\". \n\tUnexpected key(s) in state_dict: \"coarse.0.conv2d.weight\", \"coarse.0.conv2d.bias\", \"coarse.0.mask_conv2d.weight\", \"coarse.0.mask_conv2d.bias\", \"coarse.1.conv2d.weight\", \"coarse.1.conv2d.bias\", \"coarse.1.mask_conv2d.weight\", \"coarse.1.mask_conv2d.bias\", \"coarse.2.conv2d.weight\", \"coarse.2.conv2d.bias\", \"coarse.2.mask_conv2d.weight\", \"coarse.2.mask_conv2d.bias\", \"coarse.3.conv2d.weight\", \"coarse.3.conv2d.bias\", \"coarse.3.mask_conv2d.weight\", \"coarse.3.mask_conv2d.bias\", \"coarse.4.conv2d.weight\", \"coarse.4.conv2d.bias\", \"coarse.4.mask_conv2d.weight\", \"coarse.4.mask_conv2d.bias\", \"coarse.5.conv2d.weight\", \"coarse.5.conv2d.bias\", \"coarse.5.mask_conv2d.weight\", \"coarse.5.mask_conv2d.bias\", \"coarse.6.conv2d.weight\", \"coarse.6.conv2d.bias\", \"coarse.6.mask_conv2d.weight\", \"coarse.6.mask_conv2d.bias\", \"coarse.7.conv2d.weight\", \"coarse.7.conv2d.bias\", \"coarse.7.mask_conv2d.weight\", \"coarse.7.mask_conv2d.bias\", \"coarse.8.conv2d.weight\", \"coarse.8.conv2d.bias\", \"coarse.8.mask_conv2d.weight\", \"coarse.8.mask_conv2d.bias\", \"coarse.9.conv2d.weight\", \"coarse.9.conv2d.bias\", \"coarse.9.mask_conv2d.weight\", \"coarse.9.mask_conv2d.bias\", \"coarse.10.conv2d.weight\", \"coarse.10.conv2d.bias\", \"coarse.10.mask_conv2d.weight\", \"coarse.10.mask_conv2d.bias\", \"coarse.11.conv2d.weight\", \"coarse.11.conv2d.bias\", \"coarse.11.mask_conv2d.weight\", \"coarse.11.mask_conv2d.bias\", \"coarse.12.gated_conv2d.conv2d.module.bias\", \"coarse.12.gated_conv2d.conv2d.module.weight_u\", \"coarse.12.gated_conv2d.conv2d.module.weight_v\", \"coarse.12.gated_conv2d.conv2d.module.weight_bar\", \"coarse.12.gated_conv2d.mask_conv2d.module.bias\", \"coarse.12.gated_conv2d.mask_conv2d.module.weight_u\", \"coarse.12.gated_conv2d.mask_conv2d.module.weight_v\", \"coarse.12.gated_conv2d.mask_conv2d.module.weight_bar\", \"coarse.13.conv2d.weight\", \"coarse.13.conv2d.bias\", \"coarse.13.mask_conv2d.weight\", \"coarse.13.mask_conv2d.bias\", \"coarse.14.gated_conv2d.conv2d.module.bias\", \"coarse.14.gated_conv2d.conv2d.module.weight_u\", \"coarse.14.gated_conv2d.conv2d.module.weight_v\", \"coarse.14.gated_conv2d.conv2d.module.weight_bar\", \"coarse.14.gated_conv2d.mask_conv2d.module.bias\", \"coarse.14.gated_conv2d.mask_conv2d.module.weight_u\", \"coarse.14.gated_conv2d.mask_conv2d.module.weight_v\", \"coarse.14.gated_conv2d.mask_conv2d.module.weight_bar\", \"coarse.15.conv2d.weight\", \"coarse.15.conv2d.bias\", \"coarse.15.mask_conv2d.weight\", \"coarse.15.mask_conv2d.bias\", \"coarse.16.conv2d.weight\", \"coarse.16.conv2d.bias\", \"coarse.16.mask_conv2d.weight\", \"coarse.16.mask_conv2d.bias\", \"refine_conv.0.conv2d.weight\", \"refine_conv.0.conv2d.bias\", \"refine_conv.0.mask_conv2d.weight\", \"refine_conv.0.mask_conv2d.bias\", \"refine_conv.1.conv2d.weight\", \"refine_conv.1.conv2d.bias\", \"refine_conv.1.mask_conv2d.weight\", \"refine_conv.1.mask_conv2d.bias\", \"refine_conv.2.conv2d.weight\", \"refine_conv.2.conv2d.bias\", \"refine_conv.2.mask_conv2d.weight\", \"refine_conv.2.mask_conv2d.bias\", \"refine_conv.3.conv2d.weight\", \"refine_conv.3.conv2d.bias\", \"refine_conv.3.mask_conv2d.weight\", \"refine_conv.3.mask_conv2d.bias\", \"refine_conv.4.conv2d.weight\", \"refine_conv.4.conv2d.bias\", \"refine_conv.4.mask_conv2d.weight\", \"refine_conv.4.mask_conv2d.bias\", \"refine_conv.5.conv2d.weight\", \"refine_conv.5.conv2d.bias\", \"refine_conv.5.mask_conv2d.weight\", \"refine_conv.5.mask_conv2d.bias\", \"refine_conv.6.conv2d.weight\", \"refine_conv.6.conv2d.bias\", \"refine_conv.6.mask_conv2d.weight\", \"refine_conv.6.mask_conv2d.bias\", \"refine_conv.7.conv2d.weight\", \"refine_conv.7.conv2d.bias\", \"refine_conv.7.mask_conv2d.weight\", \"refine_conv.7.mask_conv2d.bias\", \"refine_conv.8.conv2d.weight\", \"refine_conv.8.conv2d.bias\", \"refine_conv.8.mask_conv2d.weight\", \"refine_conv.8.mask_conv2d.bias\", \"refine_conv.9.conv2d.weight\", \"refine_conv.9.conv2d.bias\", \"refine_conv.9.mask_conv2d.weight\", \"refine_conv.9.mask_conv2d.bias\", \"refine_atten_1.0.conv2d.weight\", \"refine_atten_1.0.conv2d.bias\", \"refine_atten_1.0.mask_conv2d.weight\", \"refine_atten_1.0.mask_conv2d.bias\", \"refine_atten_1.1.conv2d.weight\", \"refine_atten_1.1.conv2d.bias\", \"refine_atten_1.1.mask_conv2d.weight\", \"refine_atten_1.1.mask_conv2d.bias\", \"refine_atten_1.2.conv2d.weight\", \"refine_atten_1.2.conv2d.bias\", \"refine_atten_1.2.mask_conv2d.weight\", \"refine_atten_1.2.mask_conv2d.bias\", \"refine_atten_1.3.conv2d.weight\", \"refine_atten_1.3.conv2d.bias\", \"refine_atten_1.3.mask_conv2d.weight\", \"refine_atten_1.3.mask_conv2d.bias\", \"refine_atten_1.4.conv2d.weight\", \"refine_atten_1.4.conv2d.bias\", \"refine_atten_1.4.mask_conv2d.weight\", \"refine_atten_1.4.mask_conv2d.bias\", \"refine_atten_1.5.conv2d.weight\", \"refine_atten_1.5.conv2d.bias\", \"refine_atten_1.5.mask_conv2d.weight\", \"refine_atten_1.5.mask_conv2d.bias\", \"refine_atten_2.0.conv2d.weight\", \"refine_atten_2.0.conv2d.bias\", \"refine_atten_2.0.mask_conv2d.weight\", \"refine_atten_2.0.mask_conv2d.bias\", \"refine_atten_2.1.conv2d.weight\", \"refine_atten_2.1.conv2d.bias\", \"refine_atten_2.1.mask_conv2d.weight\", \"refine_atten_2.1.mask_conv2d.bias\", \"refine_combine.0.conv2d.weight\", \"refine_combine.0.conv2d.bias\", \"refine_combine.0.mask_conv2d.weight\", \"refine_combine.0.mask_conv2d.bias\", \"refine_combine.1.conv2d.weight\", \"refine_combine.1.conv2d.bias\", \"refine_combine.1.mask_conv2d.weight\", \"refine_combine.1.mask_conv2d.bias\", \"refine_combine.2.gated_conv2d.conv2d.module.bias\", \"refine_combine.2.gated_conv2d.conv2d.module.weight_u\", \"refine_combine.2.gated_conv2d.conv2d.module.weight_v\", \"refine_combine.2.gated_conv2d.conv2d.module.weight_bar\", \"refine_combine.2.gated_conv2d.mask_conv2d.module.bias\", \"refine_combine.2.gated_conv2d.mask_conv2d.module.weight_u\", \"refine_combine.2.gated_conv2d.mask_conv2d.module.weight_v\", \"refine_combine.2.gated_conv2d.mask_conv2d.module.weight_bar\", \"refine_combine.3.conv2d.weight\", \"refine_combine.3.conv2d.bias\", \"refine_combine.3.mask_conv2d.weight\", \"refine_combine.3.mask_conv2d.bias\", \"refine_combine.4.gated_conv2d.conv2d.module.bias\", \"refine_combine.4.gated_conv2d.conv2d.module.weight_u\", \"refine_combine.4.gated_conv2d.conv2d.module.weight_v\", \"refine_combine.4.gated_conv2d.conv2d.module.weight_bar\", \"refine_combine.4.gated_conv2d.mask_conv2d.module.bias\", \"refine_combine.4.gated_conv2d.mask_conv2d.module.weight_u\", \"refine_combine.4.gated_conv2d.mask_conv2d.module.weight_v\", \"refine_combine.4.gated_conv2d.mask_conv2d.module.weight_bar\", \"refine_combine.5.conv2d.weight\", \"refine_combine.5.conv2d.bias\", \"refine_combine.5.mask_conv2d.weight\", \"refine_combine.5.mask_conv2d.bias\", \"refine_combine.6.conv2d.weight\", \"refine_combine.6.conv2d.bias\", \"refine_combine.6.mask_conv2d.weight\", \"refine_combine.6.mask_conv2d.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dc4cb683b48c>\u001b[0m in \u001b[0;36m<cell line: 115>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-dc4cb683b48c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(image_path, model_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Initialize and load the pre-trained generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2190\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.2.weight\", \"encoder.2.bias\", \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.6.weight\", \"encoder.6.bias\", \"encoder.8.weight\", \"encoder.8.bias\", \"middle.0.weight\", \"middle.0.bias\", \"middle.2.weight\", \"middle.2.bias\", \"middle.4.weight\", \"middle.4.bias\", \"middle.6.weight\", \"middle.6.bias\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\", \"decoder.4.weight\", \"decoder.4.bias\", \"decoder.6.weight\", \"decoder.6.bias\". \n\tUnexpected key(s) in state_dict: \"coarse.0.conv2d.weight\", \"coarse.0.conv2d.bias\", \"coarse.0.mask_conv2d.weight\", \"coarse.0.mask_conv2d.bias\", \"coarse.1.conv2d.weight\", \"coarse.1.conv2d.bias\", \"coarse.1.mask_conv2d.weight\", \"coarse.1.mask_conv2d.bias\", \"coarse.2.conv2d.weight\", \"coarse.2.conv2d.bias\", \"coarse.2.mask_conv2d.weight\", \"coarse.2.mask_conv2d.bias\", \"coarse.3.conv2d.weight\", \"coarse.3.conv2d.bias\", \"coarse.3.mask_conv2d.weight\", \"coarse.3.mask_conv2d.bias\", \"coarse.4.conv2d.weight\", \"coarse.4.conv2d.bias\", \"coarse.4.mask_conv2d.weight\", \"coarse.4.mask_conv2d.bias\", \"coarse.5.conv2d.weight\", \"coarse.5.conv2d.bias\", \"coarse.5.mask_conv2d.weight\", \"coarse.5.mask_conv2d.bias\", \"coarse.6.conv2d.weight\", \"coarse.6.conv2d.bias\", \"coarse.6.mask_conv2d.weight\", \"coarse.6.mask_conv2d.bias\", \"coarse.7.conv2d.weight\", \"coarse.7.conv2d.bias\", \"coarse.7.mask_conv2d.weight\", \"coarse.7.mask_conv2d.bias\", \"coarse.8.conv2d.weight\", \"coarse.8.conv2d.bias\", \"coarse.8.mask_conv2d.weight\", \"coarse.8.mask_conv2d.bias\", \"coarse.9.conv2d.weight\", \"coarse.9.conv2d.bias\", \"coarse.9.mask_conv2d.weight\", \"coarse.9.mask_conv2d.bias\", \"coarse.10.conv2d.weight\", \"coarse.10.conv2d.bias\", \"coarse.10.mask_conv2d.weight\", \"coarse.10.mask_conv2d.bias\", \"coarse.11.conv2d.weight\", \"coarse.11.conv2d.bias\", \"coarse.11.mask_conv2d.weight\", \"coarse.11.mask_conv2d.bias\", \"coarse.12.gated_conv2d.conv2d.module.bias\", \"coarse.12.gated_conv2d.conv2d.module.weight_u\", \"coarse.12.gated_conv2d.conv2d.module.wei..."
          ]
        }
      ]
    }
  ]
}